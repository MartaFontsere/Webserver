#include "core/Server.hpp"
#include "cgi/CGIHandler.hpp" // For buildResponseFromCGIOutput
#include <algorithm>
#include <cstdio>       // para perror
#include <cstring>      // para memset, strerror, strlen...
#include <fcntl.h>      // para fcntl() ‚Üí modo no bloqueante
#include <iostream>     // para imprimir mensajes
#include <netinet/in.h> // para sockaddr_in
#include <poll.h>
#include <sstream>
#include <sys/socket.h> // para socket(), bind(), listen()
#include <sys/wait.h>   // para waitpid, WNOHANG
#include <unistd.h>     // para close(), read, write

#include <csignal> // para sig_atomic_t

extern volatile sig_atomic_t
    g_running; // Variable global para controlar el bucle principal
               // y determinar si el servidor debe continuar
               // ejecut√°ndose. Se define en main.cpp.

// Constructor: guarda las configuraciones (es capaz de manjar m√∫ltiples
// configuraciones de servidor virtual)
Server::Server(const std::vector<ServerConfig> &servConfigsList)
    : _servConfigsList(servConfigsList) {}
/*
El server tiene su propio vector de ServerConfig. Guardamos una copia de la
lista que nos pasan.

La referencia (&servConfigsList) solo se usa para evitar copiar dos veces
la lista de configuraciones.
*/

// Destructor: si el socket est√° abierto, lo cerramos
Server::~Server() {
  // Cerrar todos los clientes
  for (std::map<int, ClientConnection *>::iterator it = _clientsByFd.begin();
       it != _clientsByFd.end(); ++it) {
    if (it->second) {
      delete it->second; // libera la memoria del objeto Client y ademas llama
                         // al destructor de client para cerrar el fd.
    }
  }
  _clientsByFd.clear();

  // Cerrar sockets servidores
  for (size_t i = 0; i < _serverSockets.size(); ++i) {
    delete _serverSockets[i]; // esto llama al destructor de ServerSocket, que a
                              // su vez llama a closeSocket()
  }
  _serverSockets.clear();
}
/*
Un socket es un descriptor de archivo especial (como un int) que representa una
conexi√≥n de red.

En el constructor, solo guardamos el puerto (a√∫n no creamos el socket).
_serverFd se inicializa con -1 para indicar ‚Äúno hay socket abierto todav√≠a‚Äù.

En el destructor, comprobamos si el socket se cre√≥ (_serverFd != -1), y lo
cerramos para liberar recursos del sistema. Los recursos (como sockets) deben
liberarse autom√°ticamente cuando el objeto se destruye. Es la limpieza final: si
el servidor se destruye (programa acaba o objeto eliminado), hay que liberar los
recursos del sistema: cerrar sockets y liberar memoria de new Client(...).

El due√±o del file descriptor (FD) debe ser el objeto Client.
El Server crea y destruye clientes, pero no cierra sockets directamente, solo
borra los objetos Client.

Entonces:
    ClientConnection se encarga de cerrar su propio _clientFd.
    Server solo llama delete it->second;.
    No debe llamar a close() sobre los FDs.

Cuando haces:
    delete client;
ocurre exactamente esto, en orden:
    Se llama al destructor del objeto ClientConnection.
    Es decir, se ejecuta ClientConnection::~ClientConnection().
    El objeto sigue existiendo completamente durante la ejecuci√≥n del destructor
‚Äî puedes leer _clientFd, _closed, etc.

    Dentro del destructor, t√∫ puedes:
        Cerrar el socket (close(_clientFd)),
        Imprimir mensajes,
        Cambiar flags (_closed = true),
        O liberar recursos adicionales (memoria, ficheros, etc).

    Cuando termina el destructor,
    el compilador libera la memoria ocupada por el objeto.
    Solo en ese momento el puntero client ya no apunta a memoria v√°lida.

Qu√© pasa con el FD despu√©s de close()

    Cuando haces close(_clientFd):
        El descriptor de archivo (n√∫mero entero en el kernel) se libera.
        El sistema operativo puede reutilizarlo m√°s tarde para otro socket.
        Pero tu variable _clientFd sigue existiendo en el objeto Client (con el
mismo n√∫mero) hasta que el destructor termina.

    Por eso es buena pr√°ctica hacer:
        close(_clientFd);
        _clientFd = -1;

As√≠ evitas ‚Äúdoble cierre‚Äù accidental.

DUDA:
Si el flujo es:
    Detectas que un Client est√° cerrado.
    Haces delete client.
    El destructor cierra el fd (si no se cerr√≥ antes).
    El objeto desaparece.

Entonces s√≠: nadie m√°s deber√≠a acceder a ese objeto ni a ese fd.
En ese flujo limpio y lineal, no har√≠a falta ni poner closed = true ni clientFd
= -1.

‚ö†Ô∏è Pero en la pr√°ctica...
El problema es que, en servidores no bloqueantes y con m√∫ltiples pasos, no
siempre el flujo es tan lineal o ‚Äúperfecto‚Äù.

Ejemplos reales:

1. El Client puede seguir referenciado
    Aunque hagas delete client, puede que en otro punto del loop o en otra
estructura a√∫n haya punteros colgantes (por ejemplo, si ten√≠as un
std::vector<Client*> y no limpiaste bien los iteradores). üëâ Si el fd se marca a
-1, evitas intentar usar un descriptor ya cerrado.
*/

/*
std::map<int, ClientConnection> vs std::map<int, ClientConnection*>

Las dos opciones son posibles, pero cada una tiene implicaciones distintas üëá

‚úÖ Opci√≥n 1 ‚Äî std::map<int, ClientConnection>
std::map<int, ClientConnection> clients;


üëâ Aqu√≠ cada ClientConnection se guarda directamente dentro del mapa, como un
objeto completo. Ventajas:

Gesti√≥n autom√°tica de memoria (no hay new ni delete).

M√°s seguro.

Desventajas:

Si necesitas mantener punteros o referencias estables a los Client, puede
complicarse, porque el objeto puede moverse internamente si haces
inserciones/borrados.

Copiar objetos Client puede ser costoso (si son grandes).

‚úÖ Opci√≥n 2 ‚Äî std::map<int, ClientConnection*>
std::map<int, ClientConnection*> clients;


üëâ Aqu√≠ el mapa guarda punteros a objetos ClientConnection, no los objetos en s√≠.

Ventajas:

Puedes crear los clientes din√°micamente (new ClientConnection(fd)) y controlar
cu√°ndo se destruyen.

El puntero siempre es estable (no cambia aunque el mapa se modifique).

Desventajas:

Tienes que liberar manualmente la memoria (delete clientPtr) o usar punteros
inteligentes (std::unique_ptr).

Si olvidas liberar, generas fugas de memoria.

üß≠ En tu webserver (proyecto 42)

Normalmente se usa:

std::map<int, ClientConnection*> _clients;


porque:

cada cliente se asocia a un socket fd (el int),

y el servidor crea un nuevo ClientConnection din√°micamente cuando llega una
conexi√≥n:

_clients[newFd] = new ClientConnection(newFd);


luego, cuando el cliente se desconecta:

delete _clients[fd];
_clients.erase(fd);


De este modo, cada cliente tiene su propio objeto con su socket, buffer, estado,
etc.
 */

// Inicializa el servidor: crea socket, bind y listen
bool Server::init() {
  // Agrupamos las configuraciones por puerto
  std::map<int, ConfigVector> configsByPort = groupConfigsByPort();

  // Creamos un socket por cada puerto √∫nico y asociamos las configuraciones
  // correspondientes

  for (std::map<int, ConfigVector>::iterator it = configsByPort.begin();
       it != configsByPort.end(); ++it) {
    int port = it->first;
    ServerSocket *serverSocket = new ServerSocket(port);

    if (!serverSocket->init()) {
      std::cerr << "‚ùå Failed to initialize server socket on port " << port
                << std::endl;
      delete serverSocket;
      return false;
    }

    int fd = serverSocket->getFd();
    _serverSockets.push_back(serverSocket);
    _configsByServerFd[fd] = it->second;

    // A√±adimos el socket del servidor al PollManager
    _pollManager.addFd(fd, POLLIN);

    std::cout << "üåê Server listening on port " << port << " (fd: " << fd << ")"
              << std::endl;
  }

  return true;
}

// Agrupa las configuraciones por puerto
std::map<int, std::vector<ServerConfig> > Server::groupConfigsByPort() {
  std::map<int, ConfigVector> configsByPort;
  for (size_t i = 0; i < _servConfigsList.size(); ++i) {
    configsByPort[_servConfigsList[i].getListen()].push_back(
        _servConfigsList[i]);
  }
  return configsByPort;
}

/*
std::map<int, std::vector<ServerConfig> > configsByPort;
Esto significa:
puerto ‚Üí lista de reglas que escuchan ah√≠

Ejemplo mental:

server { listen 8080; server_name a; }
server { listen 8080; server_name b; }
server { listen 9090; server_name c; }


Despu√©s de este bloque, el map queda as√≠:

8080 ‚Üí [ ServerConfig(a), ServerConfig(b) ]
9090 ‚Üí [ ServerConfig(c) ]

*/

// createAndBind y setNonBlocking han sido movidos a ServerSocket.cpp

/*
En nuestro servidor, necesitamos un socket que:

    Escuche conexiones en un puerto concreto (por ejemplo, 8080).

    Est√© asociado a una direcci√≥n IP (normalmente 0.0.0.0, o sea ‚Äútodas las
interfaces locales‚Äù).

    Pueda aceptar clientes que intenten conectarse a √©l.

    üëâ La funci√≥n createAndBind() se encarga de crear ese socket y vincularlo
(bind) al puerto donde escuchar√°.

Por qu√© recibe un const char *port en lugar de std::string
    Esto es simplemente por compatibilidad con funciones de C antiguas.
    socket(), bind(), htons() y atoi() son funciones de la librer√≠a C, no de
C++.

    atoi() (convertir cadena a n√∫mero) espera un const char *.

    As√≠ que cuando en el constructor del servidor hacemos _serverFd =
createAndBind(_port.c_str());
    ... lo que estamos haciendo es convertir el std::string a const char* para
que lo pueda usar atoi().

La funci√≥n crea y configura el socket para escuchar conexiones:
    Crear socket(), configurar SO_REUSEADDR, preparar sockaddr_in y bind() al
puerto solicitado. Devuelve el descriptor o -1 en error.

Explicacion l√≠nea por l√≠nea:

socket(AF_INET, SOCK_STREAM, 0)
‚Üí Crea un socket TCP IPv4 (orientado a conexi√≥n).
    AF_INET = familia de direcciones IPv4.
    SOCK_STREAM = tipo de socket orientado a conexi√≥n (TCP).
    0: protocolo por defecto (TCP).
Si devuelve -1, algo fall√≥ (no se pudo reservar el socket).

setsockopt(... SO_REUSEADDR ...)
‚Üí Permite reiniciar el servidor sin esperar a que el puerto se libere (evita
‚ÄúAddress already in use‚Äù). Esta parte permite reutilizar el puerto
inmediatamente si reinicias el servidor. Sin esto, si paras y arrancas r√°pido,
el SO podr√≠a decir:

    ‚ÄúAddress already in use‚Äù üò©

    Porque el puerto sigue en estado TIME_WAIT unos segundos tras cerrar el
socket.

    üí° SO_REUSEADDR le dice al kernel:

    ‚ÄúTranquilo, s√© lo que hago, d√©jame reutilizar el puerto enseguida‚Äù.

    ***Pero porque deberia reiniciarse el servidor?
        Este punto (el de setsockopt(... SO_REUSEADDR ...)) suele parecer m√°gico
o innecesario al principio‚Ä¶ pero en realidad tiene que ver con c√≥mo funciona el
sistema operativo, no solo con tu c√≥digo.

        üß© 1Ô∏è‚É£ Qu√© pasa cuando tu servidor arranca

        Cuando haces esto:

        int sockfd = socket(...);
        bind(sockfd, ...);
        listen(sockfd, ...);

        El sistema operativo (Linux, macOS, etc.) reserva el puerto que le has
indicado. Por ejemplo, si pides el puerto 8080, el sistema dice:

        ‚ÄúVale, el proceso X est√° usando el puerto 8080, nadie m√°s puede usarlo
mientras siga abierto.‚Äù

        As√≠ evita conflictos (dos programas intentando escuchar en el mismo
puerto).

        üß© 2Ô∏è‚É£ Qu√© pasa cuando cierras el servidor

        Cuando terminas tu programa (o lo paras con Ctrl+C), en teor√≠a ese
socket deber√≠a cerrarse y liberar el puerto. Pero el sistema operativo no lo
libera de inmediato ‚ö†Ô∏è

        ¬øPor qu√©?
        Porque en una conexi√≥n TCP, hay un mecanismo de seguridad para
asegurarse de que no se pierdan mensajes pendientes. Cuando cierras el socket,
las conexiones que ten√≠a abiertas entran en un estado llamado TIME_WAIT.

        üîé En ese estado:

        El puerto sigue ‚Äúreservado‚Äù durante unos segundos (a veces 30‚Äì60).

        Aunque tu proceso ya termin√≥, el kernel mantiene esa reserva temporal.

        El resultado es que si intentas reiniciar el servidor inmediatamente
(por ejemplo, compilas y lo vuelves a ejecutar enseguida), te salta este error:

        Error: bind() failed
        Address already in use

        üß© 3Ô∏è‚É£ Qu√© significa ‚Äúreiniciar el servidor‚Äù

        No es que tu c√≥digo se ‚Äúreinicie solo‚Äù.
        Reiniciar significa algo como:

        T√∫ paras el programa (Ctrl+C, o matas el proceso).

        Lo vuelves a ejecutar enseguida (por ejemplo, porque has recompilado
para probar algo nuevo).

        Ejemplo pr√°ctico:

        $ ./webserv
        # Servidor escuchando en el puerto 8080...

        # Lo detienes:
        ^C   # (Ctrl+C)

        # Lo vuelves a ejecutar:
        $ ./webserv
        Error: bind() failed: Address already in use

        üí• Este error se da porque el sistema operativo a√∫n tiene el puerto 8080
bloqueado en TIME_WAIT.

        üß© 4Ô∏è‚É£ Qu√© hace setsockopt(SO_REUSEADDR)

        Esa llamada es una configuraci√≥n opcional del socket, y su funci√≥n es
decirle al sistema:

        ‚ÄúTranquilo, quiero reutilizar el puerto incluso si est√° en TIME_WAIT.‚Äù

        Es decir:
        ‚úÖ Permite volver a hacer bind() sobre el mismo puerto aunque el SO crea
que ‚Äúa√∫n est√° en uso‚Äù por una conexi√≥n previa del mismo programa.

        No afecta a la seguridad ni al funcionamiento normal.
        Solo acelera el ciclo de desarrollo y evita que tengas que esperar medio
minuto cada vez que haces un cambio en el c√≥digo.

        Si no estoy en TIME_WAIT, ¬øpara qu√© quiero SO_REUSEADDR? ¬øNo hace nada,
o incluso puede fastidiar algo?‚Äù

            üëâ No, no molesta, y s√≠ conviene dejarla siempre.
            En la mayor√≠a de casos no cambia nada cuando el puerto est√° libre, y
solo act√∫a cuando lo necesitas (cuando est√° ocupado en TIME_WAIT).

            El sistema operativo simplemente ignora la opci√≥n porque no tiene
nada que ‚Äúreutilizar‚Äù. El bind() funciona igual que siempre, sin efectos
secundarios.

            ‚úÖ As√≠ que no pasa absolutamente nada diferente respecto a no haber
puesto la l√≠nea.

Se llena la estructura sockaddr_in con:

    sin_family: AF_INET ‚Üí familia IPv4

    sin_addr.s_addr: INADDR_ANY ‚Üí escucha en cualquier IP local, es decir,
escuchar√° en 127.0.0.1, 192.168.x.x, etc.

    sin_port: htons() ‚Üí convierte el n√∫mero de puerto al formato de red (big
endian).

bind() ‚Üí asocia el socket al puerto del sistema operativo.

üí° Si bind() falla, puede ser porque ya hay otro programa usando ese puerto.

*** Explicaci√≥n m√°s en profundidad:

sockaddr_in es una estructura de C (no de C++) que describe una direcci√≥n de red
IPv4. Est√° definida en el archivo: #include <netinet/in.h> Su definici√≥n
simplificada es m√°s o menos as√≠: struct sockaddr_in { sa_family_t    sin_family;
// Familia de direcciones (AF_INET) in_port_t      sin_port;   // Puerto (en
formato network byte order) struct in_addr sin_addr;   // Direcci√≥n IP (tambi√©n
en formato network byte order) unsigned char  sin_zero[8]; // Relleno (no se
usa, pero mantiene el tama√±o)
    };

üîπ Qu√© representa
    Piensa que un socket es como un enchufe universal, pero para que el sistema
operativo sepa a qu√© puerto y a qu√© IP quieres enchufarte, tienes que darle una
direcci√≥n completa.

    üß† As√≠ que sockaddr_in ‚âà ‚Äútarjeta con la direcci√≥n postal del servidor‚Äù:
        sin_family = tipo de direcci√≥n (por ejemplo, IPv4 o IPv6).
        sin_port = puerto donde escuchas (ej. 8080).
        sin_addr = IP donde quieres escuchar (ej. 127.0.0.1 o 0.0.0.0).

üîπ Por qu√© la necesitamos
    Las funciones del sistema (como bind(), connect(), sendto(), etc.) son muy
antiguas, vienen del mundo C, y todas esperan recibir un puntero gen√©rico a una
direcci√≥n: struct sockaddr*

    Pero nosotros usamos la versi√≥n m√°s espec√≠fica:
        struct sockaddr_in

    As√≠ que cuando la pasamos a una funci√≥n, tenemos que hacer un cast:
        (struct sockaddr*)&addr
                ***Explicaci√≥n: struct sockaddr_in addr; crea una estructura
sockaddr_in, que sirve para guardar la direcci√≥n IP y el puerto cuando trabajas
con IPv4. Tu variable addr es un sockaddr_in, pero la funci√≥n espera un
sockaddr*. Entonces necesitamos hacer un casteo Esto significa: &addr ‚Üí
direcci√≥n de memoria de la variable addr (un puntero a sockaddr_in) (struct
sockaddr*) ‚Üí le decimos al compilador: ‚ÄúTranquilo, trata este puntero como si
apuntara a una sockaddr gen√©rica.‚Äù No cambia los datos en memoria, solo la forma
en que los interpretamos.

    Esto es porque la funci√≥n no sabe si le est√°s pasando una direcci√≥n IPv4
(sockaddr_in), IPv6 (sockaddr_in6), o Unix domain socket (sockaddr_un). El cast
solo le dice: ‚Äútranquilo, es del tipo gen√©rico sockaddr*, pero realmente
contiene una direcci√≥n IPv4‚Äù.

    üß† Ejemplo: el bloque real de c√≥digo
        struct sockaddr_in addr;
        addr.sin_family = AF_INET; // IPv4
        addr.sin_addr.s_addr = INADDR_ANY; // Escucha en todas las interfaces
        addr.sin_port = htons(port); // Puerto (convertido a formato de red)

    üß© Explicaci√≥n l√≠nea a l√≠nea
        1Ô∏è‚É£ addr.sin_family = AF_INET;

        Le decimos que es una direcci√≥n IPv4 (no IPv6).
        Este valor (AF_INET) est√° definido en <sys/socket.h>.

        üí° Si usaras IPv6, pondr√≠as AF_INET6.

        2Ô∏è‚É£ addr.sin_addr.s_addr = INADDR_ANY;

        Esto significa:
            ‚ÄúEscucha en todas las interfaces disponibles.‚Äù

        Si tu m√°quina tiene varias IPs (por ejemplo, una interna y otra
externa), con INADDR_ANY el servidor aceptar√° conexiones desde cualquiera.

        üí¨ Alternativas:
            Si quisieras escuchar solo en localhost, pondr√≠as:
                addr.sin_addr.s_addr = inet_addr("127.0.0.1");

        Si quisieras una IP concreta, tambi√©n podr√≠as convertirla con
inet_addr("192.168.1.42").

        3Ô∏è‚É£ addr.sin_port = htons(port);

        port aqu√≠ es el n√∫mero de puerto que t√∫ decides (por ejemplo, 8080).

        Pero ‚Äîmuy importante‚Äî el sistema operativo no guarda los n√∫meros igual
que tu CPU. Las CPUs pueden ser little endian o big endian, y eso afecta al
orden de los bytes.

        üí° Ejemplo:
            Puerto 8080 = 0x1F90

        En memoria en un Intel (little endian) se guarda como 90 1F.
        En red (network order, big endian) debe ser 1F 90.
        Por eso usamos:
            htons()  // host to network short

        Para convertir autom√°ticamente al formato correcto antes de pasar el
valor al sistema.

    4Ô∏è‚É£ ¬øY el bind()?

        Una vez has rellenado addr, haces:
            bind(sockfd, (struct sockaddr*)&addr, sizeof(addr))

        Esto le dice al sistema operativo:
            ‚ÄúAsocia mi socket (identificado por sockfd) con esta direcci√≥n IP y
este puerto.‚Äù

        Sin esto, el socket no est√° ‚Äúanclado‚Äù a ninguna direcci√≥n, y el sistema
no sabr√≠a qu√© conexiones deben llegarle.

*/

/*
Por defecto, un socket en Linux es bloqueante.
üö´ Qu√© significa ‚Äúbloqueante‚Äù
    Un socket bloqueante detiene la ejecuci√≥n del programa hasta que la
operaci√≥n termina.

    Por ejemplo:
        int client_fd = accept(server_fd, ...);

    üëâ Si no hay ning√∫n cliente intentando conectarse, esta l√≠nea se queda
esperando indefinidamente.

    Lo mismo ocurre con:
        recv() ‚Üí espera hasta que haya datos.

        send() ‚Üí espera si el buffer est√° lleno.

    Esto est√° bien si tu programa solo maneja una conexi√≥n a la vez.
    Pero si est√°s escribiendo un servidor multiprop√≥sito, como tu webserv, eso
ser√≠a un desastre: mientras una conexi√≥n est√° ‚Äúesperando‚Äù, las dem√°s se quedan
congeladas.

‚öôÔ∏è Qu√© hace ‚Äúmodo no bloqueante‚Äù

    Cuando el socket est√° en modo no bloqueante, esas funciones (accept, recv,
send, etc.) no bloquean el flujo del programa.

        Si no hay nada que aceptar, accept() devuelve -1 e errno se pone en
EAGAIN o EWOULDBLOCK.

        Si no hay datos disponibles en recv(), pasa lo mismo.

        T√∫ puedes seguir ejecutando el resto de tu c√≥digo (por ejemplo, atender
otros sockets).

    Esto es esencial para usar poll, select, o epoll ‚Äî mecanismos que te dicen
cu√°ndo un socket est√° listo para leer o escribir, sin quedarte bloqueado.

Esta funcion hace que el socket no bloquee.

fcntl(fd, F_GETFL, 0) obtiene las flags actuales del descriptor fd.

fcntl(fd, F_SETFL, flags | O_NONBLOCK) activa la flag O_NONBLOCK.
    No borra los anteriores. F_SETFL escribe los flags combinados con O_NONBLOCK
    Solo indica que el socket ya no bloquear√° el flujo.
    Retorna el valor de fcntl (0 en √©xito, -1 en fallo).

As√≠, si haces accept() y no hay clientes esperando, la llamada no se queda
congelada, sino que devuelve inmediatamente con un error controlable (EAGAIN o
EWOULDBLOCK). Osea, en modo non-blocking, accept(), recv() y send() no
bloquear√°n. En su lugar devolver√°n -1 y errno en EAGAIN/EWOULDBLOCK si no hay
datos/disponibilidad. poll() se usa para evitar llamadas en momentos con
posibilidad de bloqueo.

Esto ser√° esencial m√°s adelante cuando usemos poll().

******Profundizaci√≥n:
    Qu√© es fcntl
        fcntl significa file control ‚Üí control de archivos.

        Es una funci√≥n del sistema POSIX (Unix/Linux/macOS) que sirve para
modificar el comportamiento de un descriptor de archivo (file descriptor, o fd).

        Y recuerda que en Unix todo es un archivo:
            un archivo normal (de disco),

            un socket de red,

            una tuber√≠a (pipe),

            incluso el teclado o la pantalla‚Ä¶

    Todos se manejan con descriptores de archivo (int).

*/

/*
Qu√© tienes hasta ahora

    Has creado un objeto Server capaz de:

    Crear un socket TCP.

    Asociarlo a un puerto.

    Escuchar conexiones sin bloquear.

    Cerrar todo ordenadamente al destruir el objeto.

‚û°Ô∏è Todav√≠a no acepta clientes ni responde datos, pero ya es un servidor
inicializado que escucha. Lo siguiente ser√° crear un main.cpp que lo use y
a√±adir el bucle principal (aceptar conexiones y enviar un ‚ÄúHello world‚Äù).*/

// VERSION 2 DEL BUCLE RUN
void Server::run() {
  std::cout << "Servidor corriendo con poll()...\n" << std::endl;

  while (g_running) {
    int ready = _pollManager.wait(5000);
    // antes tenia -1, significa que espera indefinidamente
    // hasta que algo ocurra. Pongo 5 segundos de timeout, evita
    // que el servidor se quede bloqueado indefinidamente y
    // permite revisar timeouts peri√≥dicamente
    // ready es el n√∫mero de fds listos para lectura
    if (ready < 0) {
      if (errno == EINTR)
        continue;
      perror("poll");
      break;
    }
    // actualizamos el tiempo actual
    time_t now = time(NULL);

    // El vector de poll tiene una estructura fija al inicio y din√°mica despu√©s:
    // 1. Los primeros FDs ([0] a [_serverSockets.size() - 1]) son siempre los
    //    sockets servidores que escuchan nuevas conexiones.
    // 2. A partir de ah√≠, el resto de FDs son din√°micos y pueden ser:
    //    - Sockets de clientes conectados.
    //    - Pipes de salida de procesos CGI en ejecuci√≥n.

    // _serverSockets.size() es el n√∫mero de sockets servidores
    // _pollManager.getSize() es el n√∫mero total de fds

    // FASE 1: Revisar sockets servidores para aceptar nuevas conexiones
    for (size_t i = 0; i < _serverSockets.size(); ++i) {
      short revents = _pollManager.getRevents(i);
      if (revents & POLLIN) {
        acceptNewClient(_pollManager.getFd(i));
      }
    }

    // FASE 2: Procesar clientes existentes - Recorremos el resto revisando
    // todos los clientes Y los pipes CGI
    for (size_t i = _serverSockets.size(); i < _pollManager.getSize();) {
      int fd = _pollManager.getFd(i);

      // ====== CGI PIPE CHECK ======
      // Si este FD es un pipe CGI, manejarlo de forma especial
      std::map<int, ClientConnection *>::iterator cgiIt =
          _cgiPipeToClient.find(fd);
      if (cgiIt != _cgiPipeToClient.end()) {
        ClientConnection *client = cgiIt->second;
        short revents = _pollManager.getRevents(i);
        if (revents & (POLLIN | POLLHUP | POLLERR)) {
          handleCGIPipe(fd, client);
        }
        ++i;
        continue; // No procesar como socket de cliente normal
      }

      // ====== REGULAR CLIENT SOCKET ======
      ClientConnection *client = _clientsByFd[fd];

      // üßπ Si no hay cliente asociado, limpiamos el fd del poll
      if (!client) {
        _pollManager.removeFd(fd);
        continue;
      }

      // ‚úÖ VERIFICACI√ìN DE TIMEOUT PRIMERO (M√ÅS EFICIENTE)
      checkClientTimeout(client, fd, now);

      // Si el cliente sigue activo, procesar eventos
      if (!client->isClosed()) {
        // Errores de conexi√≥n
        short revents = _pollManager.getRevents(i);
        if (revents & (POLLERR | POLLHUP | POLLNVAL)) {
          client->markClosed();
          i++; // No te saltas clientes porque cleanupClosedClients() se ejecuta
               // despu√©s del bucle completo
          continue;
        }
        // Lectura de datos
        if (revents & POLLIN) {
          handleClientData(client, i);
        }
        // Escritura de datos
        if (revents & POLLOUT) {
          handleClientWrite(client, i);
          // Si se cerr√≥ durante la escritura (flushWrite marc√≥ closed si era
          // error/EOF), pasar al siguiente
          if (client->isClosed()) {
            ++i;
            continue;
          }
        }
      }

      // Solo incrementar si no borramos el elemento
      if (i < _pollManager.getSize() && _pollManager.getFd(i) == fd) {
        // Si el cliente tiene datos pendientes en el buffer (Pipelining),
        // no incrementamos 'i' para volver a procesarlo en la misma vuelta
        // o al menos asegurar que se revise.
        if (client && !client->isClosed() &&
            client->isRequestComplete() == false &&
            client->hasPendingWrite() == false) {
          // Si acabamos de terminar una request y queda data, intentamos
          // procesar la siguiente inmediatamente
          // (O simplemente dejamos que el bucle vuelva a pasar por aqu√≠)
        }
        ++i;
      }
    }

    // Limpiar clientes cerrados
    cleanupClosedClients();
  }
}

/*
17.11.25
Ahora, cuando entramos en handle client, dentro se gestiona que si hay algun
error salga de handle y siga con el bucle. El problema, es que si por ejemplo
hay un cliente con datos disponibles para leer y tambi√©n tenia datos pendientes
para escribir (o el socket es write ready, que casi siempre lo es aunque no
tengas pendientes) tendr√° ambos revents, POLLIN y POLLOUT, por lo que despu√©s de
handle client podr√≠a entrar en el for de POLLOUT y llamar a flusWrite sobre un
cliente ya cerrado dentro de handle client. No es super grave, porque el propio
fluswrite marcar√≠a closed y saldria al encontrar problemas en el send, pero es
innecesario llegar hasta ah√≠

Recordatorio r√°pido: poll() puede devolver varios flags a la vez
    poll() no te da ‚Äúun √∫nico evento‚Äù. Un mismo revents puede contener POLLIN,
POLLOUT, POLLHUP, POLLERR, etc. al mismo tiempo. Eso significa que en la misma
iteraci√≥n puedes tener que: leer datos (POLLIN), escribir datos pendientes
(POLLOUT), y adem√°s haber recibido un HUP/ERR as√≠ncrono.

    Esa simultaneidad es el origen de la necesidad de orden y cuidado.

2) Qu√© pasaba antes (tu c√≥digo original)
    Orden en cada fd:
        if (POLLIN) -> handleClientEvent(fd)
        if (POLLOUT) -> flushWrite()
        if (POLLERR|POLLHUP|POLLNVAL) -> markClosed()

    Problema real posible:
        poll() devuelve POLLIN | POLLOUT (y quiz√° tambi√©n HUP/ERR).

        En handleClientEvent() detectas un error (por ejemplo recv() devolvi√≥ 0)
y ejecutas client->markClosed() ‚Äî marcando ya _closed = true.

        Sin comprobar isClosed(), sigues y llegas a la secci√≥n if (POLLOUT) y
llamas a flushWrite() sobre un cliente ya marcado como cerrado.

        flushWrite() intentar√° send() y probablemente falle (EPIPE, ECONNRESET),
volver√° false, marcar√° _closed (otra vez) y al final cleanupClosedClients()
borrar√° el cliente.

    Consecuencias:
        Llamadas innecesarias a send() sobre sockets que ya deber√≠as considerar
muertos.

        Logs duplicados y flujos inconsistentes.

        En casos m√°s complejos (keep-alive, borrado inmediato) podr√≠a provocar
manejar √≠ndices/pollfds inv√°lidos si borras dentro del loop sin cuidado.

3) ¬øPor qu√© comprobar errores antes de lectura/escritura?
    Porque hay errores que ocurren entre tus syscalls: el peer puede cerrar o
resetear la conexi√≥n justo despu√©s del √∫ltimo send() que hiciste, y antes de la
siguiente llamada. poll() refleja ese estado as√≠ncrono con POLLERR/POLLHUP. Si
procesas I/O sin mirar primero esos flags, puedes: intentar recv() o send()
sobre un fd en mal estado, generar errores evitables, hacer trabajo in√∫til.

    Mirar los flags de error primero evita todo eso: detectas ‚Äúesto est√° roto‚Äù y
lo marcas para limpieza sin tocarlo.

    Es decir, errores as√≠ncronos (HUP/ERR) se gestionan antes de tocar el
socket, as√≠ no intentas I/O en un fd con problemas.

4) ¬øPor qu√© mover ++i al final (o controlarlo manualmente)?
    En la versi√≥n nueva gestionas i manualmente (incrementas en cada rama con
++i cuando proceda) para poder continue y no incrementar en ramas donde ya
hiciste erase. Es una forma segura de iterar cuando en algunas ramas haces
erase() del vector _pollFds. Antes ten√≠as un for (i=1; i<_pollFds.size(); ++i) y
en las ramas llamabas erase() seguido de continue. Eso tambi√©n funcionaba porque
en el continue evitabas el ++i, y la iteraci√≥n volv√≠a a comprobar el nuevo
_pollFds[i]. La versi√≥n nueva simplemente hace expl√≠cito el control del i para
evitar confusiones cuando a√±ades condiciones continue en varios puntos ‚Äî es m√°s
f√°cil razonar y menos propenso a errores sutiles.

 */

/*
ACTUALIZACI√ìN 2 informaci√≥n:

    for (size_t i = 1; i < _pollFds.size(); ++i)
    {
        int fd = _pollFds[i].fd;
        Client *client = _clientsByFd[fd];
        if (!client)
            continue;

Aunque ya est√° bien protegido en cleanupClosedClients, no deber√≠a haber fds sin
un cliente activo asociado, lo ponemos por doble seguridad. En sistemas de red,
puede haber peque√±as desincronizaciones:

    si un cliente se borra justo despu√©s de un poll() pero antes de procesar sus
eventos;

    o si ocurre un error no controlado entre readRequest() y
cleanupClosedClients();

    o si en el futuro agregas threads o funciones que manipulan _clientsByFd
fuera del bucle principal.

Si entras en ese caso, es porque tienes una incoherencia:
hay un fd en _pollFds que ya no tiene su Client en _clientsByFd.
Y si simplemente haces continue, ese fd se quedar√° en _pollFds para siempre,
ocupando espacio y haciendo que poll() lo siga vigilando in√∫tilmente.

As√≠ que s√≠ ‚úÖ ‚Äî lo correcto es eliminarlo en ese punto.
*/

/*
ACTUALIZACI√ìN informaci√≥n:

Tienes un poll() configurado solo con POLLIN, algo como esto:
    pfds[i].events = POLLIN;

Entonces:
    poll() te avisa solo cuando hay algo que leer (datos entrantes).

    Si haces un send() parcial (no todo el buffer se env√≠a) dentro de
handleClientEvent(), y te queda algo pendiente en _writeBuffer, no volver√°s a
saber cu√°ndo continuar.

üëâ Porque poll() solo te despierta con POLLIN, y no con POLLOUT

Qu√© pasa si un cliente tiene escritura pendiente

Imagina esto:
    El cliente env√≠a una petici√≥n ‚Üí poll() te despierta con POLLIN.

    En handleClientEvent() lees todo, generas respuesta, llamas a
sendResponse().

    flushWrite() intenta enviar los bytes.
        Si todo sale, genial, fin.

        Pero si send() devuelve EAGAIN ‚Üí guardas el resto en _writeBuffer.

Ahora tienes datos pendientes‚Ä¶
Pero el socket no se marca solo como POLLOUT.
As√≠ que no volver√°s a entrar para terminar de enviar hasta que el cliente vuelva
a escribir algo. Y probablemente no lo har√° ‚Üí se queda colgado esperando tu
respuesta completa.

Qu√© deber√≠a pasar (con POLLOUT activado)
    Cuando detectas que una respuesta ha quedado pendiente (hasPendingWrite() ==
true), le dices al poll():

    ‚ÄúOye, tambi√©n av√≠same cuando este socket est√© listo para escribir.‚Äù

    En c√≥digo:
        pfds[i].events |= POLLOUT;

    As√≠, en la siguiente iteraci√≥n de poll(), el kernel te despertar√° cuando el
socket tenga espacio libre en su buffer y puedas continuar enviando.

        if (pfds[i].revents & POLLIN)
        handleClientEvent(clients[i]);   // leer y preparar respuesta

        if (pfds[i].revents & POLLOUT)
        handleWriteEvent(clients[i]);    // terminar de enviar lo pendiente

Duda com√∫n:
    ‚Äú¬øNo entrar√° dos veces (una por POLLIN y otra por POLLOUT) en la misma
vuelta?‚Äù

        S√≠, puede ocurrir ‚Äî y de hecho es lo correcto ‚úÖ

        Porque un socket puede estar listo para leer y escribir al mismo tiempo.

        Por ejemplo:
            POLLIN: el cliente envi√≥ otra petici√≥n.

            POLLOUT: todav√≠a tienes datos pendientes de la respuesta anterior.

        üëâ Pero eso no es un problema.
        En esa iteraci√≥n simplemente procesas ambos eventos:
            lees lo que haya (handleClientEvent) y luego intentas escribir
(flushWrite).

        Lo que debes cuidar es el orden l√≥gico:
            Siempre lee primero (POLLIN) ‚Äî as√≠ vac√≠as el buffer de entrada.

            Luego escribe (POLLOUT) ‚Äî as√≠ respondes cuando haya espacio libre.

Qu√© pasa cuando terminas de escribir todo
    En cuanto flushWrite() termina y ya no hay nada pendiente:

    if (!client.hasPendingWrite())
        pfds[i].events &= ~POLLOUT; // desactiva inter√©s en escritura

    As√≠, el poll() ya no seguir√° avis√°ndote por POLLOUT,
    hasta que haya una nueva respuesta por enviar.

    Esto mantiene el bucle eficiente y evita que poll() te despierte sin
necesidad.

****DUDA: En el caso de una sola peticion, eso activa pollin, luego envio
respuesta y se queda a medias, para la siguiente vuelta sigue activo pollin de
esa misma petici√≥n o se desactiva si no hay mas peticiones y entonces como hay
cosas pendientes se activa solo el pollout y tengo que detectarlo?

Caso: llega una √∫nica petici√≥n
    Sup√≥n este flujo paso a paso:
        Cliente conecta y env√≠a su petici√≥n HTTP.
        ‚Üí El kernel marca el socket con POLLIN porque hay datos listos para
leer.

        Tu poll() despierta (por ese POLLIN).
        ‚Üí En tu bucle lo detectas y llamas a handleClientEvent().
        ‚Üí Lees todo con recv(), generas la respuesta y llamas a sendResponse().

        sendResponse() intenta enviar con send().
            Si se env√≠a todo, no pasa nada raro: limpias buffer, fin.

            Si se queda a medias (EAGAIN / EWOULDBLOCK) ‚Üí guardas el resto en
_writeBuffer.

    Hasta aqu√≠ bien, pero ahora pasa lo que t√∫ preguntas üëá

¬øQu√© pasa con los eventos pollin y pollout despu√©s de eso?

üü© POLLIN

    Una vez que t√∫ lees todo lo que hab√≠a del socket (con recv() hasta que
devuelve EAGAIN o 0), entonces ya no queda nada en el buffer de lectura.

    Por tanto:
        El kernel deja de marcar POLLIN autom√°ticamente.

        Tu poll() ya no te avisar√° m√°s por ese socket hasta que el cliente env√≠e
m√°s datos.

    üëâ Es decir: si no hay m√°s peticiones, no volver√°s a entrar por POLLIN.

üü• POLLOUT

    Por otro lado, si en el paso anterior tu send() devolvi√≥ EAGAIN,
    el kernel te est√° diciendo b√°sicamente:

        ‚ÄúNo puedo escribir ahora, el buffer de salida del socket est√° lleno.
        Av√≠same cuando haya espacio libre.‚Äù

    Pero ojo: el kernel no activa autom√°ticamente POLLOUT.
    Tienes que dec√≠rselo t√∫, a√±adi√©ndolo al events de ese socket:
        pfds[i].events |= POLLOUT;

    Entonces, en la pr√≥xima llamada a poll(),
    el kernel te despertar√° cuando el socket vuelva a estar listo para escribir.

Qu√© ocurre en la siguiente vuelta del bucle
    Como ya no hay nada que leer (no m√°s POLLIN),
    el √∫nico motivo por el que poll() te despertar√° ser√°:

    ‚û°Ô∏è porque el socket ahora tiene espacio libre para escribir (POLLOUT).

    Entonces t√∫ detectas:
        if (pfds[i].revents & POLLOUT)
            client->flushWrite();

    Y ah√≠ env√≠as lo que te quedaba pendiente en _writeBuffer.
    Cuando terminas (ya se envi√≥ todo), haces:

        pfds[i].events &= ~POLLOUT;

    Y el socket vuelve a estar solo con POLLIN activado,
    esperando nuevas peticiones.

Entonces, si se queda a medias, ¬øel pollin se desactiva y se activa pollout?
    ‚úÖ Exactamente.
    El kernel deja de marcar POLLIN porque ya le√≠ste todo,
    y t√∫, manualmente, activas POLLOUT para que te avise cuando puedas seguir
enviando.

Resumen r√°pido

No, no todo lo hace el kernel autom√°ticamente.
üëâ El kernel activa o desactiva din√°micamente los ‚Äúrevents‚Äù,
pero no cambia tu configuraci√≥n ‚Äúevents‚Äù.
T√∫ tienes que decidir qu√© tipo de eventos quieres monitorizar en cada momento.

üß† Diferencia entre events y revents

| Campo     | Qui√©n lo maneja | Qu√© significa | | --------- | --------------- |
------------------------------------------------------------------------------------
| | `events`  | T√∫ (tu c√≥digo)  | Qu√© condiciones quieres que `poll()` vigile
(por ejemplo: `POLLIN`, `POLLOUT`, etc.) | | `revents` | El kernel       | Qu√©
condiciones **se cumplieron realmente** cuando `poll()` despert√≥. |

Cuando el socket se queda sin datos (ya le√≠ste todo)
    Despu√©s de hacer recv() y vaciar el buffer,
    el kernel simplemente ya no marcar√° POLLIN en el pr√≥ximo revents.

    Pero no tienes que quitar POLLIN de events.
    ¬øPor qu√©?
    Porque si luego el cliente te manda otra petici√≥n,
    el kernel lo detectar√° autom√°ticamente y pondr√° revents |= POLLIN otra vez.

    üëâ As√≠ que mantener POLLIN siempre activo es normal.

Cuando el socket se llena al enviar (EAGAIN)
    Si haces send() y devuelve EAGAIN, significa:
        ‚ÄúNo hay espacio ahora en el buffer de salida.‚Äù

    Aqu√≠ s√≠ tienes que actuar t√∫:
    a√±adir POLLOUT a events para que el kernel te avise cuando el socket vuelva
a estar listo. pfds[i].events |= POLLOUT;

    Entonces, cuando el socket tenga espacio libre, en la siguiente vuelta de
poll() el kernel pondr√°: pfds[i].revents |= POLLOUT;

Cuando terminas de enviar todo
    En flushWrite(), cuando confirmas que ya no queda nada pendiente
(!hasPendingWrite()):

    T√∫ misma debes quitar el flag POLLOUT de events:
        pfds[i].events &= ~POLLOUT;

    ¬øPor qu√©?
        Porque si lo dejas activo, el kernel te seguir√° ‚Äúdespertando‚Äù por
POLLOUT todo el rato, ya que los sockets TCP casi siempre est√°n listos para
escribir. Te har√≠a gastar CPU innecesariamente.

Entonces‚Ä¶
    üëâ POLLIN: lo activas una vez y lo dejas siempre.
    El kernel decide si hay algo que leer o no, y pone/quita en revents seg√∫n
toque. No tienes que cambiarlo t√∫.

    üëâ POLLOUT: lo activas y desactivas manualmente seg√∫n el estado de tu
_writeBuffer.

*/

/* EXPLICACION

Actualizar la clase Server
Hasta ahora, tu Server:
    Crea el socket.
    Lo asocia a un puerto (bind).
    Empieza a escuchar (listen).
    Acepta conexiones (accept).

Pero solo acepta una conexi√≥n y no gestiona m√∫ltiples clientes simult√°neamente.
Si aceptas un cliente, hasta que no terminas con √©l no puedes aceptar otro. Si
dos clientes se conectan casi a la vez, el segundo tendr√° que esperar hasta que
termines con el primero. Mientras tanto, tu servidor no hace nada m√°s: no puede
recibir otros mensajes ni atender m√°s sockets, porque est√°s bloqueada en el
flujo ‚Äúuno a uno‚Äù. Aunque tenga el socket como no bloqueante y el accept() no se
queda colgado esperando, porque tienes el continue, aun as√≠ tu c√≥digo no
atender√° a m√°s de un cliente a la vez porque: No guardas los clientFd para
seguir leyendo de ellos. No tienes ninguna l√≥gica que diga: ‚Äúahora voy a leer
del cliente 1‚Äù, ‚Äúahora del cliente 2‚Äù. Solo haces accept ‚Üí send ‚Üí close. Aunque
no te bloquees esperando conexiones, tampoco gestionas m√∫ltiples clientes
simult√°neamente. As√≠ que ahora toca hacerlo capaz de manejar varios clientes a
la vez, sin que uno bloquee a los dem√°s.

Para eso lo que haremos ahora es pasar el servidor a usar poll().

pollfd es una estructura definida en <poll.h> que contiene:

struct pollfd
{
    int fd;        // el descriptor de socket
    short events;  // qu√© eventos queremos vigilar (lectura, escritura...)
    short revents; // qu√© eventos ocurrieron realmente
};

Qu√© significa ‚Äúusar poll()‚Äù

    poll() permite vigilar varios file descriptors (FDs) a la vez:

    uno para el socket del servidor (esperando nuevas conexiones),

    y varios para los clientes (esperando datos que leer o que enviar).

Piensa en poll() como un vigilante que est√° atento a varios sockets a la vez y
te avisa cuando ocurre algo interesante:

    alguien quiere conectarse,

    un cliente ha mandado datos,

    un cliente se ha desconectado‚Ä¶

Entonces t√∫ puedes actuar sin quedarte bloqueada esperando.

As√≠, en cada ciclo:

    Si el socket del servidor tiene actividad ‚Üí significa que hay un nuevo
cliente que quiere conectarse‚Üí haces accept() y lo a√±ades a tu lista de pollfd.

    Si un cliente tiene actividad ‚Üí lees su petici√≥n con readRequest().

    Si la petici√≥n est√° completa ‚Üí generas una respuesta y se la env√≠as con
sendResponse().

üëâ el socket (la puerta real de comunicaci√≥n)
üëâ y poll() (el vigilante que observa esas puertas).

Vamos a crear:

    Un std::vector<pollfd> pollFds; ‚Üí lista de sockets que estamos vigilando.
        En el √≠ndice 0 pondremos el socket del servidor (el que hace listen()).
        En los siguientes, los clientes aceptados.

Cada iteraci√≥n del bucle:

    Llamamos a poll(pollFds.data(), pollFds.size(), -1)
    (espera indefinidamente hasta que haya algo que hacer).
    Recorremos pollFds:
        Si el fd es el del servidor ‚Üí hay una nueva conexi√≥n (accept()).
        Si es otro ‚Üí ese cliente ha mandado algo o est√° listo para recibir
respuesta.

**** C√ìDIGO: Explicaci√≥n l√≠nea a l√≠nea (lo esencial)

1.
Creamos un pollfd para el socket del servidor y registramos POLLIN (nos interesa
cuando haya nuevas conexiones).

Guardamos ese pollfd en _pollFds en la posici√≥n 0: convenimos que √≠ndice 0 ser√°
siempre socket servidor.

2.
poll(_pollFds.data(), _pollFds.size(), -1)
‚Üí le pasamos todos los fds a vigilar y -1 indica ‚Äúesperar indefinidamente‚Äù.
bloquea hasta que haya eventos en alguno de los fds o hasta que una se√±al
interrumpa (EINTR).

Si errno == EINTR ‚Üí reacci√≥n adecuada: volver a llamar a poll() (esto evita
terminar por un SIGALRM u otra se√±al). EINTR significa ‚ÄúInterrupted system call‚Äù
‚Üí una llamada al sistema fue interrumpida por una se√±al antes de completarse
(por ejemplo, accept(), read(), poll(), etc.). Normalmente solo implica volver a
intentarla.

Si hay otro error ‚Üí imprimimos y salimos del bucle.

ready indica cu√°ntos fds tienen revents no nulos, pero no lo usamos directamente
para optimizar el escaneo.

3. if (_pollFds.size() > 0 && (_pollFds[0].revents & POLLIN))
            acceptNewClient();

Asegura que el vector _pollFds no est√© vac√≠o (que haya al menos un socket
registrado).

_pollFds[0] ‚Üí el primer elemento del vector (tu socket del servidor).

.revents ‚Üí campo que poll() rellena con los eventos que han ocurrido.
        Cuando haces poll(_pollFds.data(), _pollFds.size(), -1); el kernel
rellena el campo revents de cada pollfd con los eventos que han sucedido (por
ejemplo, si hay datos para leer, una desconexi√≥n, un error, etc). Events lo pone
el programador, es lo que quiere vigilar ej. POLLIN para lectura, POLLOUT para
escritura). Revents lo rellena el propio poll(), significa qu√© ha pasado de
verdad (ej. si llega POLLIN, hay datos listos).

        En el caso del servidor, est√°s preguntando: ‚Äú¬øHay algo que pueda leer
ahora en el socket del servidor?‚Äù Para un socket de servidor eso no significa
‚Äúhay datos de texto o HTML‚Äù, sino si hay una nueva conexi√≥n pendiente que puedo
aceptar con accept()

POLLIN ‚Üí bandera que indica ‚Äúhay datos para leer‚Äù (en este caso, una nueva
conexi√≥n entrante).

El operador & (AND bit a bit) sirve para comprobar si el bit de POLLIN est√°
activado.

significa:
üëâ ‚ÄúSi hay al menos un socket registrado y el socket del servidor tiene un evento
POLLIN (una nueva conexi√≥n entrante), entonces acepto esa conexi√≥n.‚Äù

***Duda:  pero como se llega a saber que alguien se quiere conectar? como llega
esa informaci√≥n al fd del servidor? Cuando creas un socket de servidor,
_serverFd no es solo un n√∫mero cualquiera, es un descriptor de archivo que el
kernel asocia con tu aplicaci√≥n. listen() le dice al kernel: ‚ÄúTodas las nuevas
conexiones que lleguen al puerto X, gu√°rdalas aqu√≠ en una cola, y cuando el
programa pregunte, se la damos‚Äù. Cuando un cliente hace: connect(server_ip,
port);

    Se inicia el handshake TCP (SYN ‚Üí SYN-ACK ‚Üí ACK).
    Una vez completado, el kernel del servidor crea una entrada en la cola de
conexiones pendientes. _serverFd sigue siendo el mismo descriptor, pero ahora el
kernel sabe que hay algo ‚Äúlisto para leer‚Äù en ese descriptor: una conexi√≥n que
se puede aceptar.

üí° Por eso, en poll(), _pollFds[0].revents & POLLIN se activa:
    ‚Äúel socket tiene algo que ‚Äòleer‚Äô ‚Üí hay una conexi√≥n esperando que aceptes‚Äù

Al llamar accept():
    Saca la primera conexi√≥n de la cola.
    Crea un nuevo socket (clientFd) dedicado a ese cliente.
    _serverFd sigue existiendo y puede aceptar m√°s conexiones nuevas.

Es decir:
    _serverFd ‚Üí puerta de entrada general (escucha nuevas conexiones)
    clientFd ‚Üí puerta de entrada personalizada para ese cliente concreto

***Fin duda****

Si hay un nuevo cliente esperando a ser aceptado, entramos en acceptNewClient()

4.
for (size_t i = 1; i < _pollFds.size(); ++i)
{
    if (_pollFds[i].revents & POLLIN)
        handleClientEvent(_pollFds[i].fd);

Despues se recorren el resto de √≠ndices de la lista _pollFds, que son los
clientes ya conectados, para evaluar si hay algun revent tipo Pollin (peticiones
que haya pendientes de leer). En tal caso, se llama a handleClientEvent

5. Revisar si en el proceso ha habido clientes que se han cerrado y hay que
limpiar
}

*/

void Server::checkClientTimeout(ClientConnection *client, int fd, time_t now) {
  const int CLIENT_TIMEOUT = 30;
  if (client->isTimedOut(now, CLIENT_TIMEOUT)) {
    std::cout << "[Timeout] Cliente fd " << fd << " inactivo m√°s de "
              << CLIENT_TIMEOUT << " segundos, cerrando.\n";
    client->markClosed();
  }
}

void Server::acceptNewClient(int serverFd) {
  while (true) {
    sockaddr_in clientAddr; // almacena IP y puerto del cliente que se conecta.
    socklen_t clientLen =
        sizeof(clientAddr); // tama√±o de esa estructura, necesario para accept()
    int clientFd = accept(serverFd, (struct sockaddr *)&clientAddr, &clientLen);
    if (clientFd == -1) {
      if (errno == EAGAIN || errno == EWOULDBLOCK)
        break; // ya no hay m√°s conexiones pendientes
      perror("accept");
      break;
    }

    // Poner el socket del cliente en modo no bloqueante
    int flags = fcntl(clientFd, F_GETFL, 0);
    if (flags == -1 || fcntl(clientFd, F_SETFL, flags | O_NONBLOCK) == -1) {
      std::cerr << "Error poniendo socket de cliente en modo no bloqueante"
                << std::endl;
      close(clientFd);
      continue;
    }

    // Pass the configs associated with this serverFd to the new Client
    ClientConnection *client = new ClientConnection(
        clientFd, clientAddr, _configsByServerFd[serverFd]);
    _clientsByFd[clientFd] = client;

    _pollManager.addFd(clientFd, POLLIN);

    std::cout << "Nueva conexi√≥n (fd: " << clientFd
              << ", IP: " << client->getIp() << ")" << std::endl;
  }
}

/*
¬øPor qu√© un bucle accept()?
    poll() te dice "hay conexiones pendientes", pero puede haber m√°s de una
esperando, por eso es un bucle infinito. En non-blocking debes accept() en bucle
hasta que accept() devuelva -1 con EAGAIN (ya no hay m√°s conexiones pendientes).
Si no haces el bucle, te quedar√≠as con conexiones sin aceptar hasta la pr√≥xima
llamada a poll().

Pasos por nuevo cliente:
    accept() ‚Üí crea un nuevo socket (fd nuevo) para hablar con ese cliente. Lo
que hace accept() internamente es tomar la conexi√≥n pendiente de la cola del
kernel y rellenar clientAddr con la direcci√≥n del cliente que se conect√≥
(direcci√≥n IPv4, la IP del cliente y el puerto del cliente) y ajustar el
clientLen al tama√±o real de los datos escritos en clientAddr _serverFd ‚Üí
descriptor del socket del servidor, escuchando en alg√∫n puerto (ej. 8080).
        (struct sockaddr*)&clientAddr ‚Üí cast porque accept espera un puntero a
sockaddr gen√©rico. clientLen ‚Üí indica el tama√±o de la estructura de direcci√≥n.

        Resultado:
            Si hay una conexi√≥n pendiente ‚Üí devuelve un nuevo fd (clientFd) para
hablar con ese cliente. Si no hay ‚Üí devuelve -1 y se setea errno. Si clientFd ==
-1 y errno es EAGAIN/EWOULDBLOCK ‚Üí significa "no hay m√°s conexiones ahora" ‚Üí
rompemos el accept-loop. Puede dar esto gracias a que est√° en modo no
bloqueante. Por el contrario, perror("accept") ‚Üí cualquier otro error real lo
imprime en consola.

    Convertimos clientFd a non-blocking.
        Esto permite que no se bloquee cuando intentemos leer o escribir datos
en ese socket m√°s adelante. Fundamental para poder atender muchos clientes a la
vez con un solo hilo.

    Creamos Client* c = new Client(clientFd); y lo guardamos en
_clientsByFd[clientFd]. Client ‚Üí clase que encapsula informaci√≥n del cliente
(fd, IP, buffer de lectura, etc.). Se guarda en _clientsByFd con clave clientFd.
        As√≠ puedes acceder r√°pidamente al cliente seg√∫n su descriptor de socket.
        üí° Nota: se usa puntero (Client*) para no copiar la clase y poder
manejarla din√°micamente.

    struct pollfd pfd = {clientFd, POLLIN, 0};
    _pollFds.push_back(pfd);
        pollfd ‚Üí estructura que poll() necesita.
        fd ‚Üí el descriptor del cliente.
        events ‚Üí eventos que queremos vigilar, aqu√≠ POLLIN (datos listos para
leer). revents ‚Üí inicializado a 0, lo rellena poll() luego. A√±adimos el nuevo
socket (fd) a la lista _pollFds para que poll() empiece a vigilar este cliente
tambi√©n.

***DUDA: _clientsByFd y _pollFds sirven para cosas distintas y complementarias.
    Cuando aceptas un cliente
        accept() te da un clientFd
        Lo guardas en _pollFds para que poll() lo vigile
        Lo guardas tambi√©n en _clientsByFd para poder acceder a su objeto
despu√©s

Cuando poll() te dice ‚Äúhay algo en fd = 7‚Äù, solo sabes que ah√≠ hay datos, pero √∫
necesitas el objeto cliente que representa ese fd, para llamar a sus funciones.
Por eso en handleClientEvent(fd) haces Client* client = _clientsByFd[fd]; Y
ahora puedes: leer del socket (client->readRequest()) enviar respuesta
(client->sendResponse()) actualizar _lastActivity etc.

Cuando detectas que un cliente cerr√≥ su conexi√≥n o que hay error, tienes que
eliminarlo de ambos sitios

As√≠ liberas memoria y evitas que poll() siga vigilando un socket muerto.

*/

void Server::handleClientData(ClientConnection *client, size_t pollIndex) {
  // 1. Leer datos del socket
  if (!client->readRequest())
    return; // error o desconexi√≥n del cliente ‚Üí cleanup lo limpiar√°->
            // marc√≥ closed

  // 2. Si la petici√≥n est√° completa, procesar la request y generar la respuesta
  // + enviar la respuesta
  if (client->isRequestComplete()) {
    if (!client->processRequest() || !client->sendResponse())
      return; // Error -> ya marc√≥ closed, cleanup lo limpiar√° despu√©s

    // === CGI ASYNC REGISTRATION ===
    if (client->getCGIState() == CGI_RUNNING) {
      int pipeFd = client->getCGIPipeFd();
      if (pipeFd != -1 &&
          _cgiPipeToClient.find(pipeFd) == _cgiPipeToClient.end()) {
        _pollManager.addFd(pipeFd, POLLIN);
        _cgiPipeToClient[pipeFd] = client;
      }
    }
  }

  // 3. Si queda algo por enviar, activar POLLOUT para que handleClientWrite
  // termine el trabajo
  if (client->hasPendingWrite()) {
    _pollManager.updateEvents(pollIndex, POLLIN | POLLOUT);
  }
}

void Server::handleClientWrite(ClientConnection *client, size_t pollIndex) {
  // Intentar vaciar el buffer de salida
  if (!client->flushWrite())
    return; // Error -> se marc√≥ closed y cleanup lo limpiar√°

  // Actualizar eventos -> Si ya no queda nada pendiente por enviar, desactivar
  // POLLOUT
  if (!client->hasPendingWrite()) {
    _pollManager.updateEvents(pollIndex, POLLIN);
  }
}
/*
Solo podemos detectar si el cliente se ha cerrado por su lado en el momento de
intentar leer (recv()) o de intentar escribir (send()), y eso solo pasa en
readrequest y en fluswrite, por lo tanto lo checkeamos despues de ambas
funciones, pero entremedias no tiene sentido hacerlo, solo si lo hemos cerrado
nosotros expresamente por un error. El error no lo podre saber hasta que envie o
reciba algo, es normal, todos los servidores funcionan as√≠. Nunca se arrastra
sin detectarlo antes de usar el socket
*/

/*
14.11.25
Actualizaci√≥n de responsabilidades que tendr√° que hacer client:
    1. readRequest()
        Recibe bytes y los pasa al parser (HttpRequest).

    2. processRequest()
        Cuando HttpRequest dice que est√° completa ‚Üí decides qu√© respuesta toca.
        Aqu√≠ se crea/llena HttpResponse.

    3. sendResponse()
        Convierte el HttpResponse en string, lo env√≠a y resetea para siguiente
petici√≥n.
*/

/*
Para que sea mas sencillo, asignamos un puntero client que se√±ala al objeto
Client correspondiente al fd que llega como argumento. Si el cliente con ese fd
existe, se guarda su puntero en client*

if (!client->readRequest()) return;
Esta l√≠nea es clave, se llama a readRequest para:
    Se lee del socket del cliente (recv) todo lo que ha llegado hasta ahora.
    Se acumula en un buffer interno (_requestBuffer).
    Si todav√≠a no ha llegado todo el mensaje (por ejemplo, si el cliente no ha
enviado a√∫n todo el encabezado HTTP), devolvemos false y esperamos a la pr√≥xima
vez que poll() diga que hay m√°s datos. Cuando la petici√≥n est√° completa (por
ejemplo, ya se recibi√≥ el doble salto de l√≠nea \r\n\r\n que marca el final de
los headers HTTP), devuelve true.

    üëâ Si devuelve false, el servidor no responde todav√≠a, sale y solo espera m√°s
datos la pr√≥xima vez.

***DUDA: PERO SI AUN FALTA POR LLEGAR, NO TENEMOS QUE ENTRAR MAS A READREQUEST,
POR SI VENIA POR PARTES O NO HA PODIDO LEERLO TODO PORQUE EL BUFFER ERA MAS
PEQUE√ëO QUE EL TAMA√ëO DE LA PETICION? Tu intuici√≥n es totalmente correcta: el
servidor no se queda bloqueado esperando a que llegue el resto, sino que vuelve
al bucle principal. Pero eso no significa que la petici√≥n se ‚Äúolvide‚Äù: el
cliente sigue registrado y poll() lo volver√° a despertar cuando haya m√°s datos
disponibles.

    if (!client->readRequest())
        return; // a√∫n no ha llegado todo
    ‚Ä¶ significa:
        ‚ÄúA√∫n no tengo toda la petici√≥n, as√≠ que no hago nada m√°s por ahora‚Äù.

    Luego el flujo contin√∫a:
        Sales de handleClientEvent().
        El bucle principal (poll()) sigue iterando y escuchando todos los
descriptores (server y clientes). En la siguiente vuelta, cuando el cliente
mande m√°s datos, poll() marcar√° su socket con POLLIN. Entonces
handleClientEvent(fd) se volver√° a llamar autom√°ticamente para ese cliente, y
esta vez readRequest() a√±adir√° el nuevo trozo a _request.

    As√≠, poco a poco se va completando la petici√≥n.

    üëâ Esto es no bloqueante y reactivo: nunca te quedas ‚Äúesperando dentro‚Äù de
una funci√≥n.
***FIN DUDA

*/

void Server::cleanupClosedClients() {
  // Recorrer todos los clientes
  for (std::map<int, ClientConnection *>::iterator it = _clientsByFd.begin();
       it != _clientsByFd.end();) {
    int fd = it->first;                    // Obtener el descriptor de archivo
    ClientConnection *client = it->second; // Obtener el puntero al cliente

    if (client->isClosed()) {
      std::cout << "Cerrando conexi√≥n fd: " << fd << std::endl;

      // Cleanup any associated CGI pipe
      int pipeFd = client->getCGIPipeFd();
      if (pipeFd != -1) {
        std::cout << "[Server] Cleaning up CGI pipe " << pipeFd
                  << " for client fd " << fd << std::endl;
        _pollManager.removeFd(pipeFd);
        _cgiPipeToClient.erase(pipeFd);
      }

      _pollManager.removeFd(fd); // Eliminar el descriptor de archivo
      delete client;             // Liberar la memoria del cliente
      _clientsByFd.erase(it++);  // Eliminar el cliente de la lista
    } else {
      ++it;
    }
  }
}

/**
 * @brief Handles CGI pipe data when poll() detects POLLIN on a CGI pipe
 *
 * This is called when there's data to read from a running CGI process.
 * Reads available data, handles EOF (CGI done), and triggers response building.
 */
void Server::handleCGIPipe(int pipeFd, ClientConnection *client) {
  if (!client || client->getCGIState() != CGI_RUNNING) {
    return;
  }

  // Read available data from CGI pipe (non-blocking)
  bool readOk = client->readCGIOutput();

  // Check if CGI is done (EOF reached)
  if (client->getCGIState() == CGI_DONE) {
    // Reap zombie process with WNOHANG
    pid_t pid = client->getCGIPid();
    if (pid > 0) {
      int status;
      waitpid(pid, &status, WNOHANG);
    }

    // Remove CGI pipe from poll and tracking map
    _pollManager.removeFd(pipeFd);
    _cgiPipeToClient.erase(pipeFd);

    // Build HTTP response from CGI output
    CGIHandler cgiHandler;
    HttpResponse response =
        cgiHandler.buildResponseFromCGIOutput(client->getCGIBuffer());

    // Set the response in the client's write buffer
    std::string responseStr = response.buildResponse();
    client->setCGIResponse(responseStr);

    // Activate POLLOUT to send response
    int clientFd = client->getFd();
    _pollManager.updateEvents(clientFd, POLLIN | POLLOUT);
  }
  (void)readOk; // Suppress unused warning for now
}
